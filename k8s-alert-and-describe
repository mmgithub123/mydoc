function mean:
1，rate()函数：获取指定时间段的平均量,(last值-first值)/时间差s

2，predict_linear(),对曲线变化速率进行计算，起到一定的预测作用。比如当前这1个小时的磁盘可用率急剧下降，这种情况可能导致磁盘很快被写满，这时可以使用该函数，
用当前1小时的数据去预测未来几个小时的状态，实现提前告警predict_linear( node_filesystem_free_bytes{mountpoint="/"}[1h],4*3600 ) < 0   
#根据最近1小时的数据计算未来4小时的磁盘使用情况，负数就会报警

3，The following functions allow aggregating each series of a given range vector over time and return an instant vector with per-series aggregation results:
avg_over_time(range-vector): the average value of all points in the specified interval.
min_over_time(range-vector): the minimum value of all points in the specified interval.
max_over_time(range-vector): the maximum value of all points in the specified interval.
sum_over_time(range-vector): the sum of all values in the specified interval.
count_over_time(range-vector): the count of all values in the specified interval.

4，ignoreing(), on()
使用ignoreing可以在匹配时忽略某些便签。而on则用于将匹配行为限定在某些便签之内。





metric mean:
一些指标表：https://lnsyyj.github.io/2019/05/27/prometheus-node-exporter-%E7%9B%91%E6%8E%A7%E9%A1%B9/

1，container_cpu_usage_seconds_total	
counter	
秒数
该容器服务针对每个CPU累计消耗的CPU时间。如果有多个CPU，则总的CPU时间需要把各个CPU耗费的时间相加

2，container_last_seen
The container_last_seen metric enables you to query to see if the last time a metric was sent was more that X seconds ago.


3，
container_spec_memory_limit_bytes： 是分配给Pod的内存配额 有等于0的情况


4，container_memory_working_set_bytes
the amount of working set memory and it includes recently accessed memory, dirty memory, and kernel memory. 
Therefore, Working set is (lesser than or equal to) </= "usage".
The container_memory_working_set_bytes is being used for OoM decisions because it excludes cached data (Linux Page Cache)
that can be evicted in memory pressure scenarios.
So, if the container_memory_working_set_bytes is increased to the limit, it will lead to oomkill.

5，kubelet_volume_stats_available_bytes   剩余容量

6,node_cpu_seconds_total

all the CPU time spent on every core for all 10 CPU modes. Those modes are: user, system, nice, idle, iowait, guest, guest_nice, steal, soft_irq and irq

node_cpu_seconds_total{cpu="0", instance="10.92.2.181:9100", job="node-exporter", mode="idle"}
1351820.79
node_cpu_seconds_total{cpu="0", instance="10.92.2.181:9100", job="node-exporter", mode="iowait"}
969.96
node_cpu_seconds_total{cpu="0", instance="10.92.2.181:9100", job="node-exporter", mode="irq"}
0
node_cpu_seconds_total{cpu="0", instance="10.92.2.181:9100", job="node-exporter", mode="nice"}
24.13
node_cpu_seconds_total{cpu="0", instance="10.92.2.181:9100", job="node-exporter", mode="softirq"}
8103.87
node_cpu_seconds_total{cpu="0", instance="10.92.2.181:9100", job="node-exporter", mode="steal"}
0
node_cpu_seconds_total{cpu="0", instance="10.92.2.181:9100", job="node-exporter", mode="system"}
19693.23
node_cpu_seconds_total{cpu="0", instance="10.92.2.181:9100", job="node-exporter", mode="user"}

7,node_memory_MemAvailable
node_memory_MemAvailable=node_memory_MemFree + node_memory_Cached + node_memory_Buffers

8,node_vmstat_oom_kill
linux为了避免内存用尽,导致系统的卡死,会唤醒oom_killer,找出/proc/pid/oom_score值最大的进程将之kill掉,从而释放内存,来保证整个系统的的正常运行。

9，node_vmstat_pgmajfault
就是 /proc/vmstat pgmajfault  就是，启动到现在，一级页面错误数。
就是虚拟内存不存在，得从磁盘swap交换，把页面读进内存。
页面错误指当软件试图读取或写入标记为“不存在”的虚拟内存位置时发生的中断。页面错误记录了一个进程必须从硬盘上恢复的次数。
页面错误是进程中当数据不在内存而必须从磁盘检索的次数。页面错误值从进程启动的时间开始累计。

10,node_filesystem_size_bytes
就是df里的1K-blocks列

11，node_filesystem_avail_bytes
就是df的Available列

12，node_filesystem_files
total number of permitted inodes

13，node_filesystem_files_free
how many more you can have inodes

14,node_disk_reads_completed_total
iostat  里的  r/s    就是1秒内完成的读次数
node_disk_writes_completed_total
w/s
node_disk_read_bytes_total
rkB/s （有个1024倍差距，因为Prometheus单位是byte）
node_disk_written_bytes_total
wkB/s






报警项：
1，KubernetesNodeReady
kube_node_status_condition{condition="Ready",status="true"} == 0
  
The conditions field describes the status of all Running nodes. Examples of conditions include:

Node Condition	         Description
Ready	       True        if the node is healthy and ready to accept pods, False if the node is not healthy and is not accepting pods, and Unknown
                         if the node controller has not heard from the node in the last node-monitor-grace-period (default is 40 seconds)
DiskPressure	True       if pressure exists on the disk size--that is, if the disk capacity is low; otherwise False
MemoryPressure	True     if pressure exists on the node memory--that is, if the node memory is low; otherwise False
PIDPressure	   True      if pressure exists on the processes—that is, if there are too many processes on the node; otherwise False
NetworkUnavailable	True if the network for the node is not correctly configured, otherwise False

If the Status of the Ready condition remains Unknown or False for longer than the pod-eviction-timeout (an argument passed to the kube-controller-manager), 
then all the Pods on the node are scheduled for deletion by the node controller. The default eviction timeout duration is five minutes. 



2，KubernetesPodNotHealthy
min_over_time(sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"})[15m:1m]) > 0

pending-》waiting，containercreating-》running-》crashloopbackoff，unknow，terminating，error-》failed，succeed


3,KubeletTooManyPods


4, KubernetesMemoryPressure


5,ContainerKilled
time() - container_last_seen > 60
If you query prometheus for the existence of a metric, it will report stale values for some time after cAdvisor has stopped 
sending metrics for that container. The container_last_seen metric enables you to query to see if the last time a metric was sent was more that X seconds ago.
E.g. if I just did count(some_cadvisor_metric{name=my-container}), it would continue to return 1 even after the container is gone.
Instead, I can do something like time() - container_last_seen{name=my-container} > 30 to see if the container has been gone a while.

6,ContainerCpuUsage
(sum by(instance, name, pod) (rate(container_cpu_usage_seconds_total[3m])) * 100) > 80
3分钟容器使用CPU时间平均量，按instance, name, pod分组求和，×100后大于80

7，ContainerMemoryUsage
(sum by(instance, name, pod) (container_memory_working_set_bytes) / sum by(instance, name, pod) (container_spec_memory_limit_bytes > 0) * 100) > 80
使用内存占limit内存大于80%

8，KubernetesVolumeFullInFourDays
 predict_linear(kubelet_volume_stats_available_bytes[6h], 4 * 24 * 3600) < 0
 用6小时的剩余容量趋势预测，未来4天剩余容量情况
 
 9，KubernetesApiClientErrors
 (sum by(instance, job) (rate(rest_client_requests_total{code=~"(5).."}[1m])) / sum by(instance, job) (rate(rest_client_requests_total[1m]))) * 100 > 1
 apiserver  http请求500的占比
 
 10，HostClockNotSynchronising
 min_over_time(node_timex_sync_status[1m]) == 0 and node_timex_maxerror_seconds >= 16
 https://github.com/prometheus/node_exporter/pull/1850
 
 11,HostHighCpuLoad
100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[2m])) * 100) > 80
idle 空闲模式很小 换句话说，CPU负载很大了

12，HostCpuStealNoisyNeighbor
avg by(instance) (rate(node_cpu_seconds_total{mode="steal"}[5m])) * 100 > 10
被偷cpu超过10%了

13,HostOutOfMemory
node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10 
可用内存不足10%了

14，HostMemoryUnderMemoryPressure
 rate(node_vmstat_pgmajfault[1m]) > 1000
 平均1分钟已经有1000次从磁盘调虚拟内存了，说明内存压力大了
 
15，HostUnusualNetworkThroughputIn
sum by (instance) (rate(node_network_receive_bytes_total[2m])) / 1024 / 1024 > 100
网络接收数据每秒大于100M

16，HostUnusualNetworkThroughputOut
sum by (instance) (rate(node_network_transmit_bytes_total[2m])) / 1024 / 1024 > 100
网络发送数据每秒大于100M

17，HostUnusualDiskReadRate
sum by (instance) (rate(node_disk_read_bytes_total[2m])) / 1024 / 1024 > 50
磁盘读数据大于每秒50M

18，HostUnusualDiskWriteRate
sum by (instance) (rate(node_disk_written_bytes_total[2m])) / 1024 / 1024 > 50
磁盘写数据大于每秒50M

19,HostOutOfDiskSpace
(node_filesystem_avail_bytes * 100) / node_filesystem_size_bytes < 10 and ON (instance, device, mountpoint) node_filesystem_readonly == 0
挂载的文件系统快满了，就剩10%不到了

20，HostDiskWillFillIn24Hours
(node_filesystem_avail_bytes * 100) / node_filesystem_size_bytes < 10 and ON (instance, device, mountpoint) 
predict_linear(node_filesystem_avail_bytes{fstype!~"tmpfs"}[1h], 24 * 3600) < 0 and ON (instance, device, mountpoint) node_filesystem_readonly == 0
预测，24小时内磁盘要满

21，HostOutOfInodes
node_filesystem_files_free{mountpoint ="/rootfs"} / node_filesystem_files{mountpoint="/rootfs"} * 100 < 10 and 
ON (instance, device, mountpoint) node_filesystem_readonly{mountpoint="/rootfs"} == 0
i节点剩不到10%了，快满了

22，HostInodesWillFillIn24Hours
node_filesystem_files_free{mountpoint ="/rootfs"} / node_filesystem_files{mountpoint="/rootfs"} * 100 < 10 and 
predict_linear(node_filesystem_files_free{mountpoint="/rootfs"}[1h], 24 * 3600) < 0 and ON (instance, device, mountpoint)  node_filesystem_readonly{mountpoint="/rootfs"} == 0
i节点预测，24小时后快满了

23，HostUnusualDiskReadLatency
rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m]) > 0.1 and rate(node_disk_reads_completed_total[1m]) > 0
rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m])   就是iostat的 r_await   一次读耗时  也就是 Disk latency 
rate(node_disk_reads_completed_total[1m])         iostat  里的  r/s
Disk latency ，is the time that it takes to complete a single I/O operation on a block device.
一次读操作大于100毫秒了，Disk latency在增长

24， HostUnusualDiskWriteLatency
rate(node_disk_write_time_seconds_total[1m]) / rate(node_disk_writes_completed_total[1m]) > 0.1 and rate(node_disk_writes_completed_total[1m]) > 0
一次写操作，大于100毫秒了

25，HostOomKillDetected
increase(node_vmstat_oom_kill[1m]) > 0
linux为了避免内存用尽,导致系统的卡死,会唤醒oom_killer,找出/proc/pid/oom_score值最大的进程将之kill掉,从而释放内存,来保证整个系统的的正常运行。

26，HostNetworkReceiveErrors
rate(node_network_receive_errs_total[2m]) / rate(node_network_receive_packets_total[2m]) > 0.01
错包占比大了

27，HostNetworkInterfaceSaturated
 (rate(node_network_receive_bytes_total{device!~"^tap.*"}[1m]) + rate(node_network_transmit_bytes_total{device!~"^tap.*"}[1m])) /
 node_network_speed_bytes{device!~"^tap.*"} > 0.8
收发流量比理论带宽，大于80%了，网络跑满了

28，
















