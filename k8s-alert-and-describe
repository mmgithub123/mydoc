prometheus metric list:

node  :  https://node.demo.do.prometheus.io/metrics

容器：
https://github.com/google/cadvisor/blob/master/docs/storage/prometheus.md#:~:text=Prometheus%20hardware%20metrics%20%20%20%20Metric%20name,%20%20%20%208%20more%20rows%20

kube-state-metrics:
很多指标，直接在GitHub里搜索代码就可以了：
比如：https://github.com/kubernetes/kube-state-metrics/search?q=kube_daemonset_status_number_ready

https://github.com/kubernetes/kube-state-metrics/blob/master/docs
https://alexander.holbreich.org/kube-state-metrics-1-8-0-all-metrics-docu/   所有list及解释

Prometheus self：
https://host/metrics



获取方式：
Go to Prometheus > Status > Targets and get the endpoint you want, for example:

Node       = http://HOSTNAME:9100/metrics
Bitbucket  = https://BITBUCKET-SERVER:443/plugins/servlet/prometheus/metrics
Prometheus = http://PROMETHEUS-SERVER:9090/metrics
...
Execute the following command to get all metrics, values and descriptions:

wget ENDPOINT
For example:

wget http://HOSTNAME:9100/metrics


------------------------------------------------------------------------------------------------------------

promql:
https://prometheus.io/docs/prometheus/latest/querying/basics

返回结果类型：
PromQL 查询结果有下面4种类型：

即时数据 (Instant vector): 一组时间序列，每个时间序列包含一个样本，所有样本共享相同的时间戳，例如：http_requests_total
区间数据 (Range vector): 组时间序列，其中包含每个时间序列随时间的一系列数据点，例如：http_requests_total[5m]
纯量数据 (Scalar): 纯量只有一个数字，没有时序，例如：count(http_requests_total)
Instant vector - a set of time series containing a single sample for each time series, all sharing the same timestamp
Range vector - a set of time series containing a range of data points over time for each time series
Scalar - a simple numeric floating point value

匹配语法：
将标签值反向匹配，或者对正则表达式匹配标签值。如操作符：
  =：选择正好相等的字符串标签
  !=：选择不相等的字符串标签
  =~：选择匹配正则表达式的标签（或子标签）
  !=：选择不匹配正则表达式的标签（或子标签）
  
时间范围通过时间范围选择器[]进行定义。例如，通过以下表达式可以选择最近5分钟内的所有样本数据，如：http_request_total{}[5m]
除了分钟，支持的单位有 
s – 秒
m– 分钟
h – 小时
d – 天
w – 周
y – 年


子查询：
Subquery
Subquery allows you to run an instant query for a given range and resolution. The result of a subquery is a range vector.
Syntax: <instant_query> '[' <range> ':' [<resolution>] ']' [ @ <float_literal> ] [ offset <duration> ]
<resolution> is optional. Default is the global evaluation interval.



聚合操作：   聚会操作得有范围，没有范围得聚合操作没有任何意义
Aggregation operators
Prometheus supports the following built-in aggregation operators that can be used to aggregate the elements of a single instant vector,
resulting in a new vector of fewer elements with aggregated values:

sum (calculate sum over dimensions)
min (select minimum over dimensions)
max (select maximum over dimensions)
avg (calculate the average over dimensions)
group (all values in the resulting vector are 1)
stddev (calculate population standard deviation over dimensions)
stdvar (calculate population standard variance over dimensions)
count (count number of elements in the vector)
count_values (count number of elements with the same value)
bottomk (smallest k elements by sample value)
topk (largest k elements by sample value)
quantile (calculate φ-quantile (0 ≤ φ ≤ 1) over dimensions)
These operators can either be used to aggregate over all label dimensions or preserve distinct dimensions by including a without or by clause. 
These clauses may be used before or after the expression.

<aggr-op> [without|by (<label list>)] ([parameter,] <vector expression>)
or
<aggr-op>([parameter,] <vector expression>) [without|by (<label list>)]
label list is a list of unquoted labels that may include a trailing comma, i.e. both (label1, label2) and (label1, label2,) are valid syntax.

without removes the listed labels from the result vector, while all other labels are preserved the output. by does the opposite and drops labels
that are not listed in the by clause, even if their label values are identical between all elements of the vector.
parameter is only required for count_values, quantile, topk and bottomk.
count_values outputs one time series per unique sample value. Each series has an additional label. The name of that label is given by the aggregation parameter,
and the label value is the unique sample value. The value of each time series is the number of times that sample value was present.
topk and bottomk are different from other aggregators in that a subset of the input samples, including the original labels, are returned in the result vector. 
by and without are only used to bucket the input vector.
quantile calculates the φ-quantile, the value that ranks at number φ*N among the N metric values of the dimensions aggregated over. φ is provided as the 
aggregation parameter. For example, quantile(0.5, ...) calculates the median, quantile(0.95, ...) the 95th percentile.

Example:
If he metric http_requests_total had time series that fan out by application, instance, and group labels, we could calculate the total number of seen HTTP requests
per application and group over all instances via:
sum without(instance) (http_requests_total)
Which is equivalent to:
sum by (application, group) (http_requests_total)
If we are just interested in the total of HTTP requests we have seen in all applications, we could simply write:
sum(http_requests_total)
To count the number of binaries running each build version we could write:
count_values("version", build_version)
To get the 5 largest HTTP requests counts across all instances we could write:
topk(5, http_requests_total)




offset   偏移修饰器，表示多长时间之前



集合运算符
通过集合运算，可以在两个瞬时向量与瞬时向量之间进行相应的集合操作。目前，Prometheus支持以下集合运算符：

and 与操作
or 或操作
unless 排除操作
and 与操作
vector1 and vector2 进行一个与操作，会产生一个新的集合。该集合中的元素同时在 vector1 和 vector2 中都存在。

例如我们有 vector1 为 A B C，vector2 为 B C D，那么 vector1 and vector2 的结果为：B C。

or 或操作
文章首发于【陈树义】公众号，点击跳转到原文：https://mp.weixin.qq.com/s/wnudWqfafzKUoDk4ke5Npg
vector1 and vector2 进行一个或操作，会产生一个新的集合。该集合中包含 vector1 和 vector2 中的所有元素。

例如我们有 vector1 为 A B C，vector2 为 B C D，那么 vector1 or vector2 的结果为：A B C D。

unless 排除操作
vector1 and vector2 进行一个或操作，会产生一个新的集合。该集合首先取 vector1 集合的所有元素，然后排除掉所有在 vector2 中存在的元素。

例如我们有 vector1 为 A B C，vector2 为 B C D，那么 vector1 unless vector2 的结果为：A。

操作符优先级
在PromQL操作符中优先级由高到低依次为：

^
*, /, %
+, -
==, !=, <=, <, >=, >
and, unless
or






函数列表：
https://prometheus.io/docs/prometheus/latest/querying/functions/

function mean:
increase( )  就是末值减去首值
eg:某一分钟cpu使用时间 increase()[1m]=该分钟末cpu使用时间-该分钟初cpu使用时间

rate(）
eg:某一分钟cpu使用率 rate()[1m]=某一分钟cpu使用时间/60s
即 rate()=increase( )/60s
1，rate()函数：获取指定时间段的平均量,(last值-first值)/时间差s

2，predict_linear(),对曲线变化速率进行计算，起到一定的预测作用。比如当前这1个小时的磁盘可用率急剧下降，这种情况可能导致磁盘很快被写满，这时可以使用该函数，
用当前1小时的数据去预测未来几个小时的状态，实现提前告警predict_linear( node_filesystem_free_bytes{mountpoint="/"}[1h],4*3600 ) < 0   
#根据最近1小时的数据计算未来4小时的磁盘使用情况，负数就会报警

3，The following functions allow aggregating each series of a given range vector over time and return an instant vector with per-series aggregation results:
avg_over_time(range-vector): the average value of all points in the specified interval.
min_over_time(range-vector): the minimum value of all points in the specified interval.
max_over_time(range-vector): the maximum value of all points in the specified interval.
sum_over_time(range-vector): the sum of all values in the specified interval.
count_over_time(range-vector): the count of all values in the specified interval.
avg_over_time()  #指定间隔内所有点的平均值。
min_over_time()  #指定间隔中所有点的最小值。
max_over_time()  #指定间隔内所有点的最大值。
sum_over_time()  #指定时间间隔内所有值的总和

4，向量与向量之间的计算

ignoring：在匹配时忽略某些标签
on：将匹配限定在某些标签之内
group_left：如果左边标签多的话，也就是多对一，使用group_left
group_right：如果右边标签多的话，也就是一对多，使用group_right
一对一匹配（标签数量相同，排除不同的标签或限制指定的标签进行匹配）
vector1 / ignoring(code) vector2
多对一匹配和一对多
vector1 on(label) group_left vector2
vector1 on(label) group_right vector2

一对一，找到这个操作符的两边向量元素的相同元素。默认情况下，操作符的格式是vector1 [operate] vector2。如果它们有相同的标签和值，则表示相匹配。





metric mean:



一些指标表：https://lnsyyj.github.io/2019/05/27/prometheus-node-exporter-%E7%9B%91%E6%8E%A7%E9%A1%B9/

1，container_cpu_usage_seconds_total	
counter	
秒数
该容器服务针对每个CPU累计消耗的CPU时间。如果有多个CPU，则总的CPU时间需要把各个CPU耗费的时间相加

2，container_last_seen
The container_last_seen metric enables you to query to see if the last time a metric was sent was more that X seconds ago.


3，
container_spec_memory_limit_bytes： 是分配给Pod的内存配额 有等于0的情况


4，container_memory_working_set_bytes
the amount of working set memory and it includes recently accessed memory, dirty memory, and kernel memory. 
Therefore, Working set is (lesser than or equal to) </= "usage".
The container_memory_working_set_bytes is being used for OoM decisions because it excludes cached data (Linux Page Cache)
that can be evicted in memory pressure scenarios.
So, if the container_memory_working_set_bytes is increased to the limit, it will lead to oomkill.

5，kubelet_volume_stats_available_bytes   剩余容量

6,node_cpu_seconds_total

all the CPU time spent on every core for all 10 CPU modes. Those modes are: user, system, nice, idle, iowait, guest, guest_nice, steal, soft_irq and irq

node_cpu_seconds_total{cpu="0", instance="10.92.2.181:9100", job="node-exporter", mode="idle"}
1351820.79
node_cpu_seconds_total{cpu="0", instance="10.92.2.181:9100", job="node-exporter", mode="iowait"}
969.96
node_cpu_seconds_total{cpu="0", instance="10.92.2.181:9100", job="node-exporter", mode="irq"}
0
node_cpu_seconds_total{cpu="0", instance="10.92.2.181:9100", job="node-exporter", mode="nice"}
24.13
node_cpu_seconds_total{cpu="0", instance="10.92.2.181:9100", job="node-exporter", mode="softirq"}
8103.87
node_cpu_seconds_total{cpu="0", instance="10.92.2.181:9100", job="node-exporter", mode="steal"}
0
node_cpu_seconds_total{cpu="0", instance="10.92.2.181:9100", job="node-exporter", mode="system"}
19693.23
node_cpu_seconds_total{cpu="0", instance="10.92.2.181:9100", job="node-exporter", mode="user"}

7,node_memory_MemAvailable
node_memory_MemAvailable=node_memory_MemFree + node_memory_Cached + node_memory_Buffers

8,node_vmstat_oom_kill
linux为了避免内存用尽,导致系统的卡死,会唤醒oom_killer,找出/proc/pid/oom_score值最大的进程将之kill掉,从而释放内存,来保证整个系统的的正常运行。

9，node_vmstat_pgmajfault
就是 /proc/vmstat pgmajfault  就是，启动到现在，一级页面错误数。
就是虚拟内存不存在，得从磁盘swap交换，把页面读进内存。
页面错误指当软件试图读取或写入标记为“不存在”的虚拟内存位置时发生的中断。页面错误记录了一个进程必须从硬盘上恢复的次数。
页面错误是进程中当数据不在内存而必须从磁盘检索的次数。页面错误值从进程启动的时间开始累计。

10,node_filesystem_size_bytes
就是df里的1K-blocks列

11，node_filesystem_avail_bytes
就是df的Available列

12，node_filesystem_files
total number of permitted inodes

13，node_filesystem_files_free
how many more you can have inodes

14,node_disk_reads_completed_total
iostat  里的  r/s    就是1秒内完成的读次数
node_disk_writes_completed_total
w/s
node_disk_read_bytes_total
rkB/s （有个1024倍差距，因为Prometheus单位是byte）
node_disk_written_bytes_total
wkB/s

15,container_cpu_cfs_throttled_seconds_total
cpu超过limit，被Linux runtime 勒死的次数

16，容器内存指标
container_memory_cache -- Number of bytes of page cache memory.
container_memory_rss -- Size of RSS in bytes.
container_memory_swap -- Container swap usage in bytes.
container_memory_usage_bytes -- Current memory usage in bytes,       
                                including all memory regardless of
                                when it was accessed.
container_memory_max_usage_bytes -- Maximum memory usage recorded 
                                    in bytes.
container_memory_working_set_bytes -- Current working set in bytes.
container_memory_failcnt -- Number of memory usage hits limits.
container_memory_failures_total -- Cumulative count of memory 
                                   allocation failures.


container_memory_working_set_bytes，is what the OOM killer is watching for







报警项：
1，KubernetesNodeReady
kube_node_status_condition{condition="Ready",status="true"} == 0
  
The conditions field describes the status of all Running nodes. Examples of conditions include:

Node Condition	         Description
Ready	       True        if the node is healthy and ready to accept pods, False if the node is not healthy and is not accepting pods, and Unknown
                         if the node controller has not heard from the node in the last node-monitor-grace-period (default is 40 seconds)
DiskPressure	True       if pressure exists on the disk size--that is, if the disk capacity is low; otherwise False
MemoryPressure	True     if pressure exists on the node memory--that is, if the node memory is low; otherwise False
PIDPressure	   True      if pressure exists on the processes—that is, if there are too many processes on the node; otherwise False
NetworkUnavailable	True if the network for the node is not correctly configured, otherwise False

If the Status of the Ready condition remains Unknown or False for longer than the pod-eviction-timeout (an argument passed to the kube-controller-manager), 
then all the Pods on the node are scheduled for deletion by the node controller. The default eviction timeout duration is five minutes. 



2，KubernetesPodNotHealthy
min_over_time(sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"})[15m:1m]) > 0

pending-》waiting，containercreating-》running-》crashloopbackoff，unknow，terminating，error-》failed，succeed


KubernetesDaemonsetRolloutStuck
kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled * 100 < 100 or kube_daemonset_status_desired_number_scheduled - 
kube_daemonset_status_current_number_scheduled > 0
有daemonset not scheduled or not ready，和节点不匹配

KubernetesDaemonsetMisscheduled
kube_daemonset_status_number_misscheduled > 0
daemonset运行在不该运行的节点上面


3,KubeletTooManyPods


4, KubernetesMemoryPressure
kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
node has MemoryPressure condition

KubernetesDiskPressure
kube_node_status_condition{condition="DiskPressure",status="true"} == 1
node has DiskPressure condition

KubernetesOutOfDisk
kube_node_status_condition{condition="OutOfDisk",status="true"} == 1
node has OutOfDisk condition

KubernetesOutOfCapacity
sum by (node) ((kube_pod_status_phase{phase="Running"} == 1) + on(pod, namespace) group_left(node) (0 * kube_pod_info)) / 
sum(kube_node_status_allocatable_pods) by (node) * 100 > 90

count(kube_pod_status_phase{phase="Running"})  集群一共running的pod数
kube_node_status_allocatable_pods              每个节点可分配pod
0 * kube_pod_info                              化0操作，只取信息，不取指标
 (kube_pod_status_phase{phase="Running"} == 1) + on(pod, namespace) group_left(node) (0 * kube_pod_info)  正在running的pod按pod和namespace对齐，留node标签用来分组求和
 running pod数在总可分配pod数的百分比
 
 KubernetesContainerOomKiller
 容器oom killer
 
  KubernetesPersistentvolumeclaimPending
  pvc pending
  
  KubernetesVolumeOutOfDiskSpace
  Volume is almost full (< 10% left)
  
  KubernetesVolumeFullInFourDays
  预测快满
  
  KubernetesPersistentvolumeError
  pv在 Failed|Pending  状态
  
  KubernetesPodCrashLooping
  increase(kube_pod_container_status_restarts_total[1m]) > 3
  容器频繁重启
  
  KubernetesApiServerErrors
  apiserver 500 错误太多
  
  


5,ContainerKilled
time() - container_last_seen > 60
If you query prometheus for the existence of a metric, it will report stale values for some time after cAdvisor has stopped 
sending metrics for that container. The container_last_seen metric enables you to query to see if the last time a metric was sent was more that X seconds ago.
E.g. if I just did count(some_cadvisor_metric{name=my-container}), it would continue to return 1 even after the container is gone.
Instead, I can do something like time() - container_last_seen{name=my-container} > 30 to see if the container has been gone a while.

6,ContainerCpuUsage
(sum by(instance, name, pod) (rate(container_cpu_usage_seconds_total[3m])) * 100) > 80
3分钟容器使用CPU时间平均量，按instance, name, pod分组求和，×100后大于80

7，ContainerMemoryUsage
(sum by(instance, name, pod) (container_memory_working_set_bytes) / sum by(instance, name, pod) (container_spec_memory_limit_bytes > 0) * 100) > 80
容器内存利用率，容器使用内存占limit内存大于80%

8，KubernetesVolumeFullInFourDays
 predict_linear(kubelet_volume_stats_available_bytes[6h], 4 * 24 * 3600) < 0
 用6小时的剩余容量趋势预测，未来4天剩余容量情况
 
 9，KubernetesApiClientErrors
 (sum by(instance, job) (rate(rest_client_requests_total{code=~"(5).."}[1m])) / sum by(instance, job) (rate(rest_client_requests_total[1m]))) * 100 > 1
 apiserver  http请求500的占比
 
 10，HostClockNotSynchronising
 min_over_time(node_timex_sync_status[1m]) == 0 and node_timex_maxerror_seconds >= 16
 https://github.com/prometheus/node_exporter/pull/1850
 
 11,HostHighCpuLoad
100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[2m])) * 100) > 80
idle 空闲模式很小 换句话说，CPU负载很大了

12，HostCpuStealNoisyNeighbor
avg by(instance) (rate(node_cpu_seconds_total{mode="steal"}[5m])) * 100 > 10
被偷cpu超过10%了

13,HostOutOfMemory
node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10 
可用内存不足10%了

14，HostMemoryUnderMemoryPressure
 rate(node_vmstat_pgmajfault[1m]) > 1000
 平均1分钟已经有1000次从磁盘调虚拟内存了，说明内存压力大了
 
15，HostUnusualNetworkThroughputIn
sum by (instance) (rate(node_network_receive_bytes_total[2m])) / 1024 / 1024 > 100
网络接收数据每秒大于100M

16，HostUnusualNetworkThroughputOut
sum by (instance) (rate(node_network_transmit_bytes_total[2m])) / 1024 / 1024 > 100
网络发送数据每秒大于100M

17，HostUnusualDiskReadRate
sum by (instance) (rate(node_disk_read_bytes_total[2m])) / 1024 / 1024 > 50
磁盘读数据大于每秒50M

18，HostUnusualDiskWriteRate
sum by (instance) (rate(node_disk_written_bytes_total[2m])) / 1024 / 1024 > 50
磁盘写数据大于每秒50M

19,HostOutOfDiskSpace
(node_filesystem_avail_bytes * 100) / node_filesystem_size_bytes < 10 and ON (instance, device, mountpoint) node_filesystem_readonly == 0
挂载的文件系统快满了，就剩10%不到了

20，HostDiskWillFillIn24Hours
(node_filesystem_avail_bytes * 100) / node_filesystem_size_bytes < 10 and ON (instance, device, mountpoint) 
predict_linear(node_filesystem_avail_bytes{fstype!~"tmpfs"}[1h], 24 * 3600) < 0 and ON (instance, device, mountpoint) node_filesystem_readonly == 0
预测，24小时内磁盘要满

21，HostOutOfInodes
node_filesystem_files_free{mountpoint ="/rootfs"} / node_filesystem_files{mountpoint="/rootfs"} * 100 < 10 and 
ON (instance, device, mountpoint) node_filesystem_readonly{mountpoint="/rootfs"} == 0
i节点剩不到10%了，快满了

22，HostInodesWillFillIn24Hours
node_filesystem_files_free{mountpoint ="/rootfs"} / node_filesystem_files{mountpoint="/rootfs"} * 100 < 10 and 
predict_linear(node_filesystem_files_free{mountpoint="/rootfs"}[1h], 24 * 3600) < 0 and ON (instance, device, mountpoint)  node_filesystem_readonly{mountpoint="/rootfs"} == 0
i节点预测，24小时后快满了

23，HostUnusualDiskReadLatency
rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m]) > 0.1 and rate(node_disk_reads_completed_total[1m]) > 0
rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m])   就是iostat的 r_await   一次读耗时  也就是 Disk latency 
rate(node_disk_reads_completed_total[1m])         iostat  里的  r/s
Disk latency ，is the time that it takes to complete a single I/O operation on a block device.
一次读操作大于100毫秒了，Disk latency在增长

24， HostUnusualDiskWriteLatency
rate(node_disk_write_time_seconds_total[1m]) / rate(node_disk_writes_completed_total[1m]) > 0.1 and rate(node_disk_writes_completed_total[1m]) > 0
一次写操作，大于100毫秒了

25，HostOomKillDetected
increase(node_vmstat_oom_kill[1m]) > 0
linux为了避免内存用尽,导致系统的卡死,会唤醒oom_killer,找出/proc/pid/oom_score值最大的进程将之kill掉,从而释放内存,来保证整个系统的的正常运行。

26，HostNetworkReceiveErrors
rate(node_network_receive_errs_total[2m]) / rate(node_network_receive_packets_total[2m]) > 0.01
错包占比大了

27，HostNetworkInterfaceSaturated
 (rate(node_network_receive_bytes_total{device!~"^tap.*"}[1m]) + rate(node_network_transmit_bytes_total{device!~"^tap.*"}[1m])) /
 node_network_speed_bytes{device!~"^tap.*"} > 0.8
收发流量比理论带宽，大于80%了，网络跑满了

28，ContainerHighThrottleRate
rate(container_cpu_cfs_throttled_seconds_total[3m]) > 1
容器cpu超过limit了，正在被干死

29，ContainerVolumeUsage
 (1 - (sum(container_fs_inodes_free) BY (instance) / sum(container_fs_inodes_total) BY (instance))) * 100 > 80
 用I节点间接计算容器存储的资源利用率
 
 30，
 


----------------------------------------------------------------------------------------------------------------------------------------------

  
    groups:
    - name: pro-kube-state-metrics-rule.rules
      rules:

      #解读：
      kube_pod_status_phase 
      数据类型：Gauge
      取值：0或1，采集时间点是哪个状态，哪个状态就为1
      kube_pod_status_phase{namespace="default", phase="Running", pod="admin-backend-695d86b674-4wxvm"} 1
      kube_pod_status_phase{namespace="default", phase="Pending", pod="admin-backend-695d86b674-4wxvm"} 0
      含义：The pods current phase  pod当前的状态阶段
      阶段取值全集：phase=<Pending|Running|Succeeded|Failed|Unknown>
      kube_pod_status_phase{phase=~"Pending|Unknown|Failed"}:
      某个时间点，取出一个pod的状态，看是否存在于Pending|Unknown|Failed这三种状态之中，如果是则三种状态的值相加为1，不是为0
      sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"}) 
      同一个pod的"Pending|Unknown|Failed"状态求和
      但这只是一种当前状态，我们取过去15分钟内，以1分钟为间隔进行抽样，取出15分钟内的状态
      sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"})[15m:1m]
      这样每一个pod都有一个15分钟内的状态向量，对于这15分钟内的以1分钟为步长抽样的15个状态，如果全是1，就说明一直处于Pending|Unknown|Failed
      三种状态中的某一个，那么全是1，就可以表示成，最小值都大于0
      min_over_time(sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"})[15m:1m]) > 0
      #15分钟内以1分钟间隔抽样检测pod在Pending|Unknown|Failed状态，min_over_time 取这15个值的最小值，如果最小值都大于0，
      说明全是1，说明15分钟内一直就从来没有处在正常状态过。
      #pod大致流程图：pending-》waiting，containercreating-》running-》crashloopbackoff，unknow，terminating，error-》failed，succeed
      - alert: KubernetesPodNotHealthy
        expr: min_over_time(sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"})[15m:1m]) > 0
        for: 0m
        labels:
          severity: error
          env: prod
          team: op #three value op,rd,unknow
          #team: unknow #three value op,rd,unknow
        annotations:
          #summary: " {{ $_kube_pod_labels_.label_app }}"
          description: "Pod has been in a non-ready state for longer than 15 minutes.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"     
     
     
      #有daemonset和节点不匹配， not scheduled 或者是 not ready，
       kube_daemonset_status_number_ready    gauge
       The number of nodes that should be running the daemon pod and have one or more of the daemon pod running and ready.
       daemonset pod 既是running 又 ready 的节点数。
       
       kube_daemonset_status_desired_number_scheduled gauge
       The number of nodes that should be running the daemon pod.
       需要运行daemon pod 的节点数
       
      - alert: KubernetesDaemonsetRolloutStuck
        expr: kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled * 100 < 100 
        for: 10m
        labels:
          severity: error
          env: prod
          team: op #three value op,rd,unknow
          #team: unknow #three value op,rd,unknow
        annotations:
          summary: Kubernetes DaemonSet rollout stuck (instance {{ $labels.instance }})
          description: "Some Pods of DaemonSet are not scheduled or not ready\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"


      kube_daemonset_status_number_misscheduled gauge
      The number of nodes running a daemon pod but are not supposed to
      还没运行daemon pod 的节点数。
      因为在扩容时，一定有一个时间点，是还没来得及调度的，所以这时候如果报，是误报，所以加一个时间间隔，
      这个15分钟间隔，来自GitHub讨论及prometheus-operator官方：
      https://github.com/kubernetes/kube-state-metrics/issues/812
      https://github.com/kubernetes-monitoring/kubernetes-mixin/issues/347
      https://github.com/helm/charts/blob/master/stable/prometheus-operator/templates/prometheus/rules-1.14/kubernetes-apps.yaml#L149-L156
      
      - alert: KubernetesDaemonsetMisscheduled
        expr: kube_daemonset_status_number_misscheduled > 0
        for: 15m
        labels:
          severity: error
          env: prod
          team: op #three value op,rd,unknow
          #team: unknow #three value op,rd,unknow
        annotations:
          summary: Kubernetes DaemonSet misscheduled (instance {{ $labels.instance }})
          description: "Some DaemonSet Pods are running where they are not supposed to run\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      #已经10分钟不是condition="Ready",status="true"了。condition共有Ready ，DiskPressure，MemoryPressure ，
      #PIDPressure ，NetworkUnavailable  5种状态。
      #节点状态的定义：kubelet 会将一个或多个”驱逐信号“映射到对应的节点状态。如果满足硬驱逐阈值，或者满足独立于其关联宽限期的软驱逐阈值时，
      #kubelet将报告节点处于压力下的状态。
      #当可用资源较少时，kubelet需要保证节点稳定性， 如果任意一种资源耗尽，节点将会变得不稳定，所以定义一些驱逐信号，当满足信号时，做为
      #决策依据，触发驱逐行为，那么会主动终止一或多个 Pod 的运行，以回收资源。当一个 Pod 被终止时，
      #其中的容器会全部停止，Pod 状态会被置为 Failed。
      
      #节点状态根据相应的驱逐信号定义：
      #MemoryPressure	memory.available	节点上可用内存量达到逐出阈值
      #DiskPressure	nodefs.available, nodefs.inodesFree, imagefs.available, 或 imagefs.inodesFree	
      #节点或者节点的根文件系统或镜像文件系统上可用磁盘空间和 i 节点个数达到逐出阈值
      #PIDPressure	pid.available	在（Linux）节点上的可用进程标识符已降至驱逐阈值以下
      
      #kubelet 有如下所示的默认硬驱逐阈值：
      #memory.available<100Mi
      #nodefs.available<10%
      #imagefs.available<15%
      #nodefs.inodesFree<5％
      详细见：https://kubernetes.io/zh/docs/tasks/administer-cluster/out-of-resource/
      
      - alert: KubernetesNodeReady
        expr: kube_node_status_condition{condition="Ready",status="true"} == 0
        for: 10m
        labels:
          severity: critical
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Kubernetes Node ready (instance {{ $labels.instance }})
          description: "Node {{ $labels.node }} has been unready for a long time\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      #见alert: KubernetesNodeReady
      - alert: KubernetesMemoryPressure
        expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
        for: 2m
        labels:
          severity: error
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Kubernetes memory pressure (instance {{ $labels.instance }})
          description: "{{ $labels.node }} has MemoryPressure condition\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
      
      #见alert: KubernetesNodeReady
      - alert: KubernetesDiskPressure
        expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
        for: 2m
        labels:
          severity: error
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Kubernetes disk pressure (instance {{ $labels.instance }})
          description: "{{ $labels.node }} has DiskPressure condition\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"


      #意思是说running pod数在总可分配pod数的百分比
      #count(kube_pod_status_phase{phase="Running"})  集群一共running的pod数
      #kube_node_status_allocatable_pods              每个节点可分配pod
      #0 * kube_pod_info                              化0操作，只取信息，不取指标
      #(kube_pod_status_phase{phase="Running"} == 1) + on(pod, namespace) group_left(node) (0 * kube_pod_info)  
      #正在running的pod按pod和namespace对齐，留node标签用来分组求和
      - alert: KubernetesOutOfCapacity
        expr: sum by (node) ((kube_pod_status_phase{phase="Running"} == 1) + on(pod, namespace) group_left(node) (0 * kube_pod_info)) / sum(kube_node_status_allocatable_pods) by (node) * 100 > 90
        for: 2m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Kubernetes out of capacity (instance {{ $labels.instance }})
          description: "{{ $labels.node }} is out of capacity\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 容器oom killer
      #过去10分钟内，容器重启次数大于1 且最近10分钟内容器终止原因是OOMKilled
      - alert: KubernetesContainerOomKiller
        expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
        for: 0m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
          #team: unknow #three value op,rd,unknow
        annotations:
          summary: Kubernetes container oom killer (instance {{ $labels.instance }})
          description: "Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      #pvc剩余空间小于10%了
      - alert: KubernetesVolumeOutOfDiskSpace
        expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes * 100 < 10
        for: 2m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Kubernetes Volume out of disk space (instance {{ $labels.instance }})
          description: "Volume is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
      
      #当前6小时的使用情况预测，4天后PVC将满
      - alert: KubernetesVolumeFullInFourDays
        expr: predict_linear(kubelet_volume_stats_available_bytes[6h], 4 * 24 * 3600) < 0
        for: 0m
        labels:
          severity: critical
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Kubernetes Volume full in four days (instance {{ $labels.instance }})
          description: "{{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is expected to fill up within four days. Currently {{ $value | humanize }}% is available.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"



      #一分钟内重启次数大于三次，且持续了2分钟
      - alert: KubernetesPodCrashLooping
        expr: increase(kube_pod_container_status_restarts_total[1m]) > 3
        for: 2m
        labels:
          severity: error
          env: prod
          #team: unknow #three value op,rd,unknow
          team: op #three value op,rd,unknow
        annotations:
          summary: Kubernetes pod crash looping (instance {{ $labels.instance }})
          description: "Pod {{ $labels.pod }} is crash looping\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"


      #api client 500 请求占比超过 1%了
      - alert: KubernetesApiClientErrors
        expr: (sum(rate(rest_client_requests_total{code=~"(5).."}[1m])) by (instance, job) / sum(rate(rest_client_requests_total[1m])) by (instance, job)) * 100 > 1
        for: 2m
        labels:
          severity: critical
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Kubernetes API client errors (instance {{ $labels.instance }})
          description: "Kubernetes API client is experiencing high error rate\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      #证书下周过期
      - alert: KubernetesClientCertificateExpiresNextWeek
        expr: apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 7*24*60*60
        for: 0m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Kubernetes client certificate expires next week (instance {{ $labels.instance }})
          description: "A client certificate used to authenticate to the apiserver is expiring next week.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      #证书24小时内要过期
      - alert: KubernetesClientCertificateExpiresSoon
        expr: apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 24*60*60
        for: 0m
        labels:
          severity: critical
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Kubernetes client certificate expires soon (instance {{ $labels.instance }})
          description: "A client certificate used to authenticate to the apiserver is expiring in less than 24.0 hours.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

    

    - name: pro-prometheus-rule.rules
      rules:     


      - alert: PrometheusTargetMissing
        expr: up == 0
        for: 0m
        labels:
          severity: critical
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Prometheus target missing (instance {{ $labels.instance }})
          description: "A Prometheus target has disappeared. An exporter might be crashed.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusAllTargetsMissing
        expr: count by (job) (up) == 0
        for: 0m
        labels:
          severity: critical
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Prometheus all targets missing (instance {{ $labels.instance }})
          description: "A Prometheus job does not have living target anymore.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusConfigurationReloadFailure
        expr: prometheus_config_last_reload_successful != 1
        for: 0m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Prometheus configuration reload failure (instance {{ $labels.instance }})
          description: "Prometheus configuration reload error\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusTooManyRestarts
        expr: changes(process_start_time_seconds{job=~"prometheus|pushgateway|alertmanager"}[15m]) > 2
        for: 0m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Prometheus too many restarts (instance {{ $labels.instance }})
          description: "Prometheus has restarted more than twice in the last 15 minutes. It might be crashlooping.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusAlertmanagerConfigurationReloadFailure
        expr: alertmanager_config_last_reload_successful != 1
        for: 0m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Prometheus AlertManager configuration reload failure (instance {{ $labels.instance }})
          description: "AlertManager configuration reload error\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusAlertmanagerConfigNotSynced
        expr: count(count_values("config_hash", alertmanager_config_hash)) > 1
        for: 0m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Prometheus AlertManager config not synced (instance {{ $labels.instance }})
          description: "Configurations of AlertManager cluster instances are out of sync\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusAlertmanagerE2eDeadManSwitch
        expr: vector(1)
        for: 0m
        labels:
          severity: error
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Prometheus AlertManager E2E dead man switch (instance {{ $labels.instance }})
          description: "Prometheus DeadManSwitch is an always-firing alert. It's used as an end-to-end test of Prometheus through the Alertmanager.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

 
      - alert: PrometheusNotConnectedToAlertmanager
        expr: prometheus_notifications_alertmanagers_discovered < 1
        for: 0m
        labels:
          severity: critical
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Prometheus not connected to alertmanager (instance {{ $labels.instance }})
          description: "Prometheus cannot connect the alertmanager\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      
      - alert: PrometheusRuleEvaluationFailures
        expr: increase(prometheus_rule_evaluation_failures_total[3m]) > 0
        for: 0m
        labels:
          severity: critical
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Prometheus rule evaluation failures (instance {{ $labels.instance }})
          description: "Prometheus encountered {{ $value }} rule evaluation failures, leading to potentially ignored alerts.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusTemplateTextExpansionFailures
        expr: increase(prometheus_template_text_expansion_failures_total[3m]) > 0
        for: 0m
        labels:
          severity: critical
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Prometheus template text expansion failures (instance {{ $labels.instance }})
          description: "Prometheus encountered {{ $value }} template text expansion failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusRuleEvaluationSlow
        expr: prometheus_rule_group_last_duration_seconds > prometheus_rule_group_interval_seconds
        for: 5m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Prometheus rule evaluation slow (instance {{ $labels.instance }})
          description: "Prometheus rule evaluation took more time than the scheduled interval. It indicates a slower storage backend access or too complex query.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusNotificationsBacklog
        expr: min_over_time(prometheus_notifications_queue_length[10m]) > 0
        for: 0m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Prometheus notifications backlog (instance {{ $labels.instance }})
          description: "The Prometheus notification queue has not been empty for 10 minutes\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusAlertmanagerNotificationFailing
        expr: rate(alertmanager_notifications_failed_total[1m]) > 0
        for: 0m
        labels:
          severity: critical
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Prometheus AlertManager notification failing (instance {{ $labels.instance }})
          description: "Alertmanager is failing sending notifications\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusTargetEmpty
        expr: prometheus_sd_discovered_targets == 0
        for: 0m
        labels:
          severity: critical
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Prometheus target empty (instance {{ $labels.instance }})
          description: "Prometheus has no target in service discovery\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusTargetScrapingSlow
        expr: prometheus_target_interval_length_seconds{quantile="0.9"} > 60
        for: 5m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Prometheus target scraping slow (instance {{ $labels.instance }})
          description: "Prometheus is scraping exporters slowly\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusLargeScrape
        expr: increase(prometheus_target_scrapes_exceeded_sample_limit_total[10m]) > 10
        for: 5m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Prometheus large scrape (instance {{ $labels.instance }})
          description: "Prometheus has many scrapes that exceed the sample limit\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusTargetScrapeDuplicate
        expr: increase(prometheus_target_scrapes_sample_duplicate_timestamp_total[5m]) > 0
        for: 0m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Prometheus target scrape duplicate (instance {{ $labels.instance }})
          description: "Prometheus has many samples rejected due to duplicate timestamps but different values\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusTsdbCheckpointCreationFailures
        expr: increase(prometheus_tsdb_checkpoint_creations_failed_total[1m]) > 0
        for: 0m
        labels:
          severity: critical
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Prometheus TSDB checkpoint creation failures (instance {{ $labels.instance }})
          description: "Prometheus encountered {{ $value }} checkpoint creation failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusTsdbCheckpointDeletionFailures
        expr: increase(prometheus_tsdb_checkpoint_deletions_failed_total[1m]) > 0
        for: 0m
        labels:
          severity: critical
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Prometheus TSDB checkpoint deletion failures (instance {{ $labels.instance }})
          description: "Prometheus encountered {{ $value }} checkpoint deletion failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusTsdbCompactionsFailed
        expr: increase(prometheus_tsdb_compactions_failed_total[1m]) > 0
        for: 0m
        labels:
          severity: critical
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Prometheus TSDB compactions failed (instance {{ $labels.instance }})
          description: "Prometheus encountered {{ $value }} TSDB compactions failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusTsdbHeadTruncationsFailed
        expr: increase(prometheus_tsdb_head_truncations_failed_total[1m]) > 0
        for: 0m
        labels:
          severity: critical
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Prometheus TSDB head truncations failed (instance {{ $labels.instance }})
          description: "Prometheus encountered {{ $value }} TSDB head truncation failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusTsdbReloadFailures
        expr: increase(prometheus_tsdb_reloads_failures_total[1m]) > 0
        for: 0m
        labels:
          severity: critical
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Prometheus TSDB reload failures (instance {{ $labels.instance }})
          description: "Prometheus encountered {{ $value }} TSDB reload failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusTsdbWalCorruptions
        expr: increase(prometheus_tsdb_wal_corruptions_total[1m]) > 0
        for: 0m
        labels:
          severity: critical
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Prometheus TSDB WAL corruptions (instance {{ $labels.instance }})
          description: "Prometheus encountered {{ $value }} TSDB WAL corruptions\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: PrometheusTsdbWalTruncationsFailed
        expr: increase(prometheus_tsdb_wal_truncations_failed_total[1m]) > 0
        for: 0m
        labels:
          severity: critical
          env: prod
          team: op #three value op,rd,unknow
        annotations:
          summary: Prometheus TSDB WAL truncations failed (instance {{ $labels.instance }})
          description: "Prometheus encountered {{ $value }} TSDB WAL truncation failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"



    - name: pro-docker-containers.rules 
      rules:     
      
      #最后一次看到metric已经是60秒前了，认为容器已经挂了
      - alert: ContainerKilled
        expr: time() - container_last_seen > 60
        for: 0m
        labels:
          severity: error
          env: prod
          team: op #three value op,rd,unknow
          #team: unknow #three value op,rd,unknow
        annotations:
          summary: Container killed (instance {{ $labels.instance }})
          description: "A container has disappeared\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      #3分钟容器使用CPU时间平均量，按instance, name, pod分组求和，×100后大于80
      - alert: ContainerCpuUsage
        expr: (sum(rate(container_cpu_usage_seconds_total[3m])) BY (instance, name,pod) * 100) > 80
        for: 2m
        labels:
          severity: error
          env: prod
          team: op #three value op,rd,unknow
          #team: unknow #three value op,rd,unknow
        annotations:
          summary: Container CPU usage (instance {{ $labels.instance }})
          description: "Container CPU usage is above 80%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      #容器内存利用率，容器使用内存占limit内存大于80%
      - alert: ContainerMemoryUsage
        expr: (sum(container_memory_working_set_bytes) BY (instance, name,pod) / sum(container_spec_memory_limit_bytes > 0) BY (instance, name,pod) * 100) > 80
        for: 2m
        labels:
          severity: error
          env: prod
          team: op #three value op,rd,unknow
          #team: unknow #three value op,rd,unknow
        annotations:
          summary: Container Memory usage (instance {{ $labels.instance }})
          description: "Container Memory usage is above 80%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 用I节点间接计算容器存储的资源利用率 已经大于80%
      - alert: ContainerVolumeUsage
        expr: (1 - (sum(container_fs_inodes_free) BY (instance) / sum(container_fs_inodes_total) BY (instance))) * 100 > 80
        for: 2m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
          #team: unknow #three value op,rd,unknow
        annotations:
          summary: Container Volume usage (instance {{ $labels.instance }})
          description: "Container Volume usage is above 80%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      
      #容器cpu超过limit了，正在被干死
      - alert: ContainerHighThrottleRate
        expr: rate(container_cpu_cfs_throttled_seconds_total[3m]) > 1
        for: 2m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
          #team: unknow #three value op,rd,unknow
        annotations:
          summary: Container high throttle rate (instance {{ $labels.instance }})
          description: "Container is being throttled\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

     

    - name: pro-node-exporter.rules 
      rules:     
      
      #可用内存不足10%了
      - alert: HostOutOfMemory
        expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10
        for: 2m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
          #team: unknow #three value op,rd,unknow
        annotations:
          summary: Host out of memory (instance {{ $labels.instance }})
          description: "Node memory is filling up (< 10% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      #平均1分钟已经有1000次从磁盘调虚拟内存了，说明内存压力大了
      - alert: HostMemoryUnderMemoryPressure
        expr: rate(node_vmstat_pgmajfault[1m]) > 1000
        for: 2m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
          #team: unknow #three value op,rd,unknow
        annotations:
          summary: Host memory under memory pressure (instance {{ $labels.instance }})
          description: "The node is under heavy memory pressure. High rate of major page faults\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      #网络接收数据每秒大于100M
      - alert: HostUnusualNetworkThroughputIn
        expr: sum by (instance) (rate(node_network_receive_bytes_total[2m])) / 1024 / 1024 > 100
        for: 5m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
          #team: unknow #three value op,rd,unknow
        annotations:
          summary: Host unusual network throughput in (instance {{ $labels.instance }})
          description: "Host network interfaces are probably receiving too much data (> 100 MB/s)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      #网络发送数据每秒大于100M
      - alert: HostUnusualNetworkThroughputOut
        expr: sum by (instance) (rate(node_network_transmit_bytes_total[2m])) / 1024 / 1024 > 100
        for: 5m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
          #team: unknow #three value op,rd,unknow
        annotations:
          summary: Host unusual network throughput out (instance {{ $labels.instance }})
          description: "Host network interfaces are probably sending too much data (> 100 MB/s)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
   
      #磁盘读数据大于每秒50M
      - alert: HostUnusualDiskReadRate
        expr: sum by (instance) (rate(node_disk_read_bytes_total[2m])) / 1024 / 1024 > 50
        for: 5m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
          #team: unknow #three value op,rd,unknow
        annotations:
          summary: Host unusual disk read rate (instance {{ $labels.instance }})
          description: "Disk is probably reading too much data (> 50 MB/s)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
 
      #磁盘写数据大于每秒50M
      - alert: HostUnusualDiskWriteRate
        expr: sum by (instance) (rate(node_disk_written_bytes_total[2m])) / 1024 / 1024 > 50
        for: 2m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
          #team: unknow #three value op,rd,unknow
        annotations:
          summary: Host unusual disk write rate (instance {{ $labels.instance }})
          description: "Disk is probably writing too much data (> 50 MB/s)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      #挂载的文件系统快满了，就剩10%不到了
      - alert: HostOutOfDiskSpace
        expr: (node_filesystem_avail_bytes * 100) / node_filesystem_size_bytes < 10 and ON (instance, device, mountpoint) node_filesystem_readonly == 0
        for: 2m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
          #team: unknow #three value op,rd,unknow
        annotations:
          summary: Host out of disk space (instance {{ $labels.instance }})
          description: "Disk is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      #预测，24小时内磁盘要满
      - alert: HostDiskWillFillIn24Hours
        expr: (node_filesystem_avail_bytes * 100) / node_filesystem_size_bytes < 10 and ON (instance, device, mountpoint) predict_linear(node_filesystem_avail_bytes{fstype!~"tmpfs"}[1h], 24 * 3600) < 0 and ON (instance, device, mountpoint) node_filesystem_readonly == 0
        for: 2m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
          #team: unknow #three value op,rd,unknow
        annotations:
          summary: Host disk will fill in 24 hours (instance {{ $labels.instance }})
          description: "Filesystem is predicted to run out of space within the next 24 hours at current write rate\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      #i节点剩不到10%了，快满了
      - alert: HostOutOfInodes
        expr: node_filesystem_files_free{mountpoint ="/rootfs"} / node_filesystem_files{mountpoint="/rootfs"} * 100 < 10 and ON (instance, device, mountpoint) node_filesystem_readonly{mountpoint="/rootfs"} == 0
        for: 2m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
          #team: unknow #three value op,rd,unknow
        annotations:
          summary: Host out of inodes (instance {{ $labels.instance }})
          description: "Disk is almost running out of available inodes (< 10% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      #i节点预测，24小时后快满了
      - alert: HostInodesWillFillIn24Hours
        expr: node_filesystem_files_free{mountpoint ="/rootfs"} / node_filesystem_files{mountpoint="/rootfs"} * 100 < 10 and predict_linear(node_filesystem_files_free{mountpoint="/rootfs"}[1h], 24 * 3600) < 0 and ON (instance, device, mountpoint) node_filesystem_readonly{mountpoint="/rootfs"} == 0
        for: 2m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
          #team: unknow #three value op,rd,unknow
        annotations:
          summary: Host inodes will fill in 24 hours (instance {{ $labels.instance }})
          description: "Filesystem is predicted to run out of inodes within the next 24 hours at current write rate\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"


      #一次读操作大于100毫秒了，Disk latency在增长
      - alert: HostUnusualDiskReadLatency
        expr: rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m]) > 0.1 and rate(node_disk_reads_completed_total[1m]) > 0
        for: 2m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
          #team: unknow #three value op,rd,unknow
        annotations:
          summary: Host unusual disk read latency (instance {{ $labels.instance }})
          description: "Disk latency is growing (read operations > 100ms)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      #一次写操作，大于100毫秒了
      - alert: HostUnusualDiskWriteLatency
        expr: rate(node_disk_write_time_seconds_total[1m]) / rate(node_disk_writes_completed_total[1m]) > 0.1 and rate(node_disk_writes_completed_total[1m]) > 0
        for: 2m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
          #team: unknow #three value op,rd,unknow
        annotations:
          summary: Host unusual disk write latency (instance {{ $labels.instance }})
          description: "Disk latency is growing (write operations > 100ms)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      #idle 空闲模式很小 换句话说，CPU负载很大了
      - alert: HostHighCpuLoad
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[2m])) * 100) > 80
        for: 0m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
          #team: unknow #three value op,rd,unknow
        annotations:
          summary: Host high CPU load (instance {{ $labels.instance }})
          description: "CPU load is > 80%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      #被偷cpu超过10%了
      - alert: HostCpuStealNoisyNeighbor
        expr: avg by(instance) (rate(node_cpu_seconds_total{mode="steal"}[5m])) * 100 > 10
        for: 0m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
          #team: unknow #three value op,rd,unknow
        annotations:
          summary: Host CPU steal noisy neighbor (instance {{ $labels.instance }})
          description: "CPU steal is > 10%. A noisy neighbor is killing VM performances or a spot instance may be out of credit.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      #swap快满了
      - alert: HostSwapIsFillingUp
        expr: (1 - (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes)) * 100 > 80
        for: 2m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
          #team: unknow #three value op,rd,unknow
        annotations:
          summary: Host swap is filling up (instance {{ $labels.instance }})
          description: "Swap is filling up (>80%)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

    
      #host内核版本不一致
      - alert: HostKernelVersionDeviations
        expr: count(sum(label_replace(node_uname_info, "kernel", "$1", "release", "([0-9]+.[0-9]+.[0-9]+).*")) by (kernel)) > 1
        for: 6h
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
          #team: unknow #three value op,rd,unknow
        annotations:
          summary: Host kernel version deviations (instance {{ $labels.instance }})
          description: "Different kernel versions are running\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      #有 oom kill 了，linux为了避免内存用尽,导致系统的卡死,会唤醒oom_killer,找出/proc/pid/oom_score值最大的进程将之kill掉,
      #从而释放内存,来保证整个系统的的正常运行。
      - alert: HostOomKillDetected
        expr: increase(node_vmstat_oom_kill[1m]) > 0
        for: 0m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
          #team: unknow #three value op,rd,unknow
        annotations:
          summary: Host OOM kill detected (instance {{ $labels.instance }})
          description: "OOM kill detected\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      #错包占比大了
      - alert: HostNetworkReceiveErrors
        expr: rate(node_network_receive_errs_total[2m]) / rate(node_network_receive_packets_total[2m]) > 0.01
        for: 2m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
          #team: unknow #three value op,rd,unknow
        annotations:
          summary: Host Network Receive Errors (instance {{ $labels.instance }})
          description: "Host {{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} receive errors in the last five minutes.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      #收发流量比理论带宽，大于80%了，网络跑满了
      - alert: HostNetworkInterfaceSaturated
        expr:  (rate(node_network_receive_bytes_total{device!~"^tap.*"}[1m]) + rate(node_network_transmit_bytes_total{device!~"^tap.*"}[1m])) / node_network_speed_bytes{device!~"^tap.*"} > 0.8
        for: 1m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
          #team: unknow #three value op,rd,unknow
        annotations:
          summary: Host Network Interface Saturated (instance {{ $labels.instance }})
          description: "The network interface \"{{ $labels.interface }}\" on \"{{ $labels.instance }}\" is getting overloaded.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      
      #时钟不同步
      - alert: HostClockNotSynchronising
        expr: min_over_time(node_timex_sync_status[1m]) == 0 and node_timex_maxerror_seconds >= 16
        for: 2m
        labels:
          severity: warning
          env: prod
          team: op #three value op,rd,unknow
          #team: unknow #three value op,rd,unknow
        annotations:
          summary: Host clock not synchronising (instance {{ $labels.instance }})
          description:  "Clock not synchronising.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"


---------------------------------------------------------------------------------------------------------------------------------------------
PLEG 耗时高
histogram_quantile(0.99, sum(rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) by (cluster_id, instance, le) * 
on(instance, cluster_id) group_left(node) kubelet_node_name{job="kubelet"}) >= 10
for:5m
PLEG 操作耗时的99分位数超过10秒













